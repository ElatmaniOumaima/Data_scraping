{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.keys import Keys\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver import ActionChains\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from datetime import datetime\n",
    "\n",
    "\n",
    "from selenium.common.exceptions import NoSuchElementException\n",
    "import webdriver_manager\n",
    "import time\n",
    "import os\n",
    "import pandas as pd \n",
    "import numpy as np "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Plan of  indeed scraping:\n",
    "## first:\n",
    "        >>we gonna automate the whole search as follow\n",
    "        1>> get the page \n",
    "        2>> select intrested countries (country by country)\n",
    "        3><> automate keywords case\n",
    "        4><> search bar \n",
    "## second:\n",
    "       >>extract all the pages returned (loop)\n",
    "       >>iterate over links and get the infos  \n",
    "        >> Nb: u can adjust the number of page and add cookies blocker and ads blocker if u don't wanna disable or delete them manually"
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##initialize the list of intrested countries\n",
    "\n",
    "os.environ[\"PATH\"]+=r\"C:/chromedriver-win64\"\n",
    "url=\"https://ma.indeed.com/\"\n",
    "#instancier le webdriver \n",
    "driver=webdriver.Chrome()\n",
    "\n",
    "page=driver.get(url)\n",
    "#get the title \n",
    "page_title=driver.title\n",
    "print(page_title)\n",
    "time.sleep(5)\n",
    "countries= driver.find_element(By.XPATH, '//*[@id=\"gnav-footer-container\"]/div/footer/div/ul[1]/li[3]/a')\n",
    "countries.click()\n",
    "\n",
    "international=driver.find_element(By.XPATH,'//*[@id=\"page\"]/div/div/ul')\n",
    "pays=international.find_elements(By.TAG_NAME,\"li\")\n",
    "r=len(pays)#the list lenght\n",
    "print(r)#le nombre des pays existant en indeed qu'on vas y extraire the job oppotunities\n",
    "\n",
    "L=[]# initialize the dataframe\n",
    "for i in range(r):\n",
    "      royaume=pays[i].text\n",
    "    \n",
    "    \n",
    "      \n",
    "      pays[i].find_element(By.TAG_NAME,\"a\").click()\n",
    "      post=driver.find_element(By.NAME,\"q\")\n",
    "      post.click()\n",
    "      time.sleep(4)\n",
    "      post.send_keys(\"Data science,Data analytics,AI,ML,machine learning,Data scrapingn,NLP\")\n",
    "      \n",
    "      search=driver.find_element(By.XPATH,'//*[@id=\"jobsearch\"]/div/div[2]/button')\n",
    "      search.send_keys(Keys.RETURN)\n",
    "      ##pagination for every country \n",
    "      nav =driver.find_element(By.XPATH,\"//*[@id='jobsearch-JapanPage']/div/div[5]/div[1]/nav\")\n",
    "      lis3=nav.find_element(By.TAG_NAME,\"ul\")\n",
    "      lies=lis3.find_elements(By.TAG_NAME,\"li\")\n",
    "      N=20 #the number of page that we want exract \n",
    "      while True:\n",
    "            lies[-1].find_element(By.TAG_NAME,\"a\").click()\n",
    "             #when the page gonna be clicked , we exract all the links and clicked on them one by one , finaly exract the data \n",
    "            div_jobs_results=driver.find_element(By.ID,\"jobsearch-Main\")\n",
    "            job_job_list=div_jobs_results.find_element(By.XPATH,'//*[@id=\"jobsearch-JapanPage\"]/div/div[5]/div[1]')\n",
    "            data_list=job_job_list.find_element(By.XPATH,'//*[@id=\"mosaic-jobResults\"]')\n",
    "            ul=data_list.find_element(By.CSS_SELECTOR,\"#mosaic-provider-jobcards > ul\")\n",
    "            data_list2=ul.find_elements(By.CSS_SELECTOR,\"#mosaic-provider-jobcards > ul > li\")\n",
    "            #for j in range(len(data_list2)):\n",
    "            print(len(data_list2))\n",
    "            N1=0\n",
    "            while True:\n",
    "                  #get the table\n",
    "                  try:\n",
    "                        \n",
    "                        job_description=data_list2[N1].find_element(By.TAG_NAME,\"h2\").text\n",
    "                        company_name=data_list2[N1].find_element(By.CSS_SELECTOR,\"#mosaic-provider-jobcards > ul > li > div > div> div > div > div > table > tbody > tr > td > div > div > span\").text\n",
    "                        city=data_list2[N1].find_element(By.CSS_SELECTOR,\"#mosaic-provider-jobcards > ul > li > div > div > div > div > div > table> tbody > tr > td > div > div > div\").text\n",
    "                        job_type=data_list2[N1].find_elements(By.CSS_SELECTOR,\"#mosaic-provider-jobcards > ul > li > div > div > div > div> div > table> tbody > tr > td > div\")[2].text\n",
    "                        date_publication=data_list2[N1].find_element(By.CSS_SELECTOR,\"#mosaic-provider-jobcards > ul > li > div > div> div > div> div > table > tbody > tr > td > div > span.date\").text\n",
    "                        #scraping_date=\n",
    "                        job_requirement=data_list2[N1].find_element(By.CSS_SELECTOR,\"#mosaic-provider-jobcards > ul > li > div > div > div > div > div > table > tbody > tr > td > div.heading6.tapItem-gutter.result-footer > div\").text\n",
    "                        #link_for_applying=data_list2[N1].find_element(By.XPATH,\"//*[@id='applyButtonLinkContainer'']/div/div/button\")\n",
    "                        full_data=dict(nom_entreprise=company_name,ville=city,royaume1=royaume,travail_description=job_description,\n",
    "                                       type_travail=job_type,publication_date=date_publication,job_requirement1=job_requirement)\n",
    "                        L.append(full_data)      \n",
    "                  except:\n",
    "                        print(\"NA\")\n",
    "\n",
    "                  \n",
    "                  N1+=1\n",
    "                  if N1==len(data_list2):\n",
    "                        break\n",
    "                  elif N1==5:\n",
    "                        N1+=1\n",
    "            #print(pd.DataFrame(L))     \n",
    "                  \n",
    "            #we must iterate over the list elements ,and each jobs li  \"mosaic-afterFifthJobResult\" \n",
    "                      \n",
    "            time.sleep(5)\n",
    "           \n",
    "            #i put N as number of pages cuz the while loop gonna run infinittly for that we have to set a break condition and precize how many pages we want \n",
    "            N-=1\n",
    "            if N==0:\n",
    "                  break \n",
    "            nav =driver.find_element(By.XPATH,\"//*[@id='jobsearch-JapanPage']/div/div[5]/div[1]/nav\")\n",
    "            lis3=nav.find_element(By.TAG_NAME,\"ul\")\n",
    "            lies=lis3.find_elements(By.TAG_NAME,\"li\")\n",
    "            time.sleep(5)\n",
    "      page1=driver.get(\"https://ma.indeed.com/worldwide\")\n",
    "      international=driver.find_element(By.XPATH,'//*[@id=\"page\"]/div/div/ul')\n",
    "      pays=international.find_elements(By.TAG_NAME,\"li\")\n",
    "\n",
    "Data_final_exracted=pd.DataFrame(L)  \n",
    "print(Data_final_exracted)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Data_final_exracted.to_csv(\"C:/Users/Oumaima/Desktop/S3/Data_scraping_course/Selenium/Indeed.csv\")#export the actual data to workspace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
